\chapter{Background}
\label{chap:background}
\lhead{\emph{Background}}
%
%
% Notes Start
%
%
%The key question to answer in this chapter is: "What has been done/is being done". 

%This chapter comprises around 4000 words and should put your project into context within Computer Science. Your focus here should be on the final section "Current State of the Art". This should be at least 2500 of the 4000 words of this section.
%
%
% Notes End
%
%
\section{Thematic Area within Computer Science}
The Core topic of this project is safely and dynamically encrypting messages between two parties. The communication will rely on multiple functioning NodeJs servers for transfer of encrypted messages. 

The core areas under which my project falls under is cryptography, security for encrypting and securing information. Machine learning will be used for establishing methods of secure information exchange. And finally networking due to the setup required of communicating between different servers and sending encrypted information.


Encryption \cite{encryptionDefinition} is when the plaintext of any form of data that can be easily read is converted to an unreadable encoded version. In order to retrieve the original data for viewing or processing it must be decoded using a specific algorithm and more than likely some sort of key, usually a lengthy password. Encryption may be used for encrypting files and operating systems on a user's hard drive. In today's world encryption is used religiously for data transferred between networks. Sensitive information like user's credentials are constantly being sent from the browser to the server when logging into websites for personalised content. The same is true for for even more high risk information like banking details, scans of identification documents and even keys. Websites that wish to be secure are now using HTTPS instead of HTTP. The Number of websites using HTTPS is constantly increasing see figure \ref{fig:httpsRise}

\begin{figure}[ht]
  \centering
      \includegraphics[width=0.9\textwidth]{Figures/httpsRise.png}
  \caption[A graph of HTTPS usage increase]{A graph of HTTPS usage increase\cite{https}}
  \label{fig:httpsRise}
\end{figure}

HTTP is not secure because information transmitted is in plaintext by default and extra steps are needed to encrypt the data. Because the author of the server can choose how the data is encrypted, it can lead to the theft of data as the implementation may not be correct or a weak algorithm is used.
On the other hand if a website uses HTTPS which is a common defined standard there will be minimal data theft see figure \ref{fig:https1}. HTTPS uses SSL or TLS which are protocols that use asymmetric keys (will be discussed later). SSL is generally used more often as it requires the server to acquire an SSL certificate from a trusted third party.

\begin{figure}[ht]
  \centering
      \includegraphics[width=0.6\textwidth]{Figures/httpsVsHttp.png}
  \caption[HTTP vs HTTPS]{HTTP vs HTTPS\cite{https1}}
  \label{fig:https1}
\end{figure}



Traditionally there are two encryption types.
\begin{enumerate}
    \item Symmetric
    \item Asymmetric
\end{enumerate}

Symmetric encryption uses the same key to encrypt and decrypt information. This type of encryption is usually used to encrypt information provided by a human generated key. It is not safe to send this key over a network as it can be stolen or the destination being sent to can be spoofed. There are multiple implementations of symmetric ciphers, the most common being AES, Twofish and Serpent. To increase your security at the cost of encryption and decryption time you can chain multiple ciphers together AES(Twofish(Key)).

Asymmetric or commonly known as public key encryption methods are commonly used for sharing data between between computers on different networks. This is because a set of keys are generated one being private and the other public. The private key is never shared and remains on the host that generated it. The public key on the other hand can be shared with the party you want to communicate securely with. The public key is used to encrypt the data that is about to be sent back. This data can only be decrypted using the private key. Therefore you can share your public key with anyone and they wont be able to decrypt messages sent from another host who used the same public key. The most common algorithm is RSA. Certain protocols also use public key algorithms like SSH for secure remote connections to foreign hosts. And GPG for verification of packages on Linux systems and an alternative over https for Github.

Protocols like SSH create a set of keys during the start of the session and those keys remain constant therefore if the private key was leaked the whole conversation could be decrypted if the packets have been captured and stored.  

Encryption in its general form is simply a mathematical algorithm that takes plaintext and combines it with some sort of key over a number of iterations eventually producing the ciphertext. This might entice some people to try and break those ciphers and recover the original plain text. Quite a number of attacks do exist.
Private keys are sometimes stored on the disk or in temporary files that are saved by programs during their execution, until reboot or they are cleared after a number of days. The attacker may be able to access the server physically or remotely using an unrelated exploit and copy the key.

Social engineering is an attack where a human pretends to be of an authority figure and convinces an unaware human to give up the key. This can be done by an attacker pretending to be an executive engineer in a company and convince the victim indirectly or directly to give up the keys by running obfuscated commands in the terminal which then send over the key to the attackers server.

If the key used is created by a human and not some sort of machine generator there are a few number attacks that can be performed that would not be feasible or possible if the key was generated or quite long. These attacks include brute force which creates keys in usually ascending order or based on some algorithm to increase the chance of success. Brute force will eventually try every key possible however even a small sized key of 12 characters containing numbers, symbols, upper and lover case numbers it would take around 200 years \cite{brute}. 
Dictionary attacks can be used if the key is part of a large dictionary of human created passwords. 

Attacks on proper keys that are generated by machines are more sophisticated and rely on cracking the algorithm or device used for encryption more so then the key.
Linear cryptanalysis \cite{cipher-attacks} is a plaintext attack which means that the attack can use any plain text they want and receive the ciphertext for it after putting it through a system. This attack uses linear approximations to describe the behaviour of the block cipher. After large number of pairs of plaintext and cipher text there is a possibility to learn something about the key.

Algebraic attacks \cite{cipher-attacks} can be used if the ciphers exhibit a high probability of a mathematical structure. 

Reverse Engineering \cite{cipher-attacks} can be used to either examine the source code of the algorithm or disassemble the binary which uses the algorithm to look as the assembly code of the algorithm.
Machine key generators usually use some form of a random number generator which are algorithms that usually take in a seed hopefully something that isn't the current time but that has been known to be used and return a key. Attacks can be made on this number generator if the seed is something predictable or the generator generates predictable numbers. 

If the device on which the algorithm is performing on is an embedded device you can perform side channel attacks \cite{cipher-attacks} where you measure the spikes and frequency of the power consumption when the encryption is taking place. 

%
%
% End of encription 
%
%

Machine learning is a category of algorithms that allow systems to automatically learn and improve from experience without being explicitly programmed \cite{machineLearning}. Basically this means that the algorithm can update when input is received this in turn updates the the output even if the same inputs are used later on.
Typically machine learning software processes large amounts of data and looks for patterns constantly updating either variables or adding logic branches. Recommendation engines use machine learning to personalise the logged in users feed. So if user one looked at product X and user 2 bought product X and also bought product Y then user one will most likely see product Y as a recommendation. There are three types of recommendation systems. Collaborative Filtering \cite{recommendation} where similarities between customers is taken into account.
Content Based Filtering \cite{recommendation} is when the liked and purchased items are taken into account. See figure \ref{fig:recommendation} for illustration.
Finally there is Hybrid Recommendation Systems which is a mix between the previous two and is typically the one used in industry.

\begin{figure}[ht]
  \centering
      \includegraphics[width=0.9\textwidth]{Figures/recommendationEngine.png}
  \caption[An example of recommendation engine]{An example of recommendation engine\cite{recommendation}}
  \label{fig:recommendation}
\end{figure}

The most common machine learning algorithms are often classified as supervised learning and unsupervised learning.
Supervised learning \cite{supervised} relies on a set of training data which is constructed of various inputs and and correct outputs corresponding to those inputs usually labels. The training data is usually left unchanged and when the test data or data taken from the current user is used as input the algorithm attempts to correctly classify the data based on the training data. The disadvantage of supervised learning is if the incoming data is radically different from the test data the probability of classifying the data into the correct label will be similar and sometimes even lower than classifying into the incorrect label. However because the training data exists it can be quite quicker to setup a supervised learning system then an unsupervised.

Unsupervised learning \cite{unsupervised} uses data that is neither classified or labelled. This allows the algorithm to draw its own conclusions about the data as well as discover hidden structures. If some of the data is similar in any way to another piece of data the algorithm tends to group those pieces of data together. Unsupervised learning is generally more complex and can be more accurate since the algorithm might find subtle discrepancies that might be impossible to notice for a human. This however can lead to unpredictable behaviour where there is expected to be two classifications but instead the data is classified into a lot more than two classifications. Unsupervised learning algorithms typically require more training data than supervised in order to detect similarities and come up with labels.

There are also algorithms that use techniques found in both supervised and unsupervised machine learning these are called Semi-supervised \cite{machineLearning0} machine learning these use a much larger amount of unlabelled training data than labelled. These types of algorithms usually tend to be more accurate than supervised and unsupervised alone.

Another interesting suite of machine learning algorithms is classified into Reinforcement Learning \cite{reinforcementLearning}. These algorithms interact with the environment and received rewards or penalties based on the actions carried out within the environment. If the action carried out receives a reward it is likely that the same action will be repeated again. 

%
%
% End of machine learning
%
%

Achieving synchronisation \cite{sync} is an important part of this project. 
Synchronisation is the process of making two or more data storage devices or programs (in the same or different computers) having exactly the same information at a given time.
Synchronisation is common in multi-threaded software where any number of threads can manipulate the same set of data or even wait for a certain thread to finish execution. For example you wouldn't want your parent thread to finish before the child threads as this can lead to data loss and resources being hogged by a zombie thread. Pthread \cite{pthread} is a great API that can be used for handling threads and shared data between those threads. 
Data Synchronisation \cite{datasync} is an on going process of having the same data present on selected servers. Large databases that operate on multiple servers usually use data versioning to keep check on the latest data. MongoDB is one such database \cite{mongo} this is the reason for a delay when you upload a YouTube video (YouTube doesn't use MongoDB this is just an example) it might not be instantly available for other users to watch as it needs to propagate through a number of servers hosting the databases.
Popular websites use data synchronisation for mirroring websites. This allows users to be distributed among relatively identical servers in order to avoid bandwidth bottlenecks as well as increasing availability just in case one server dies users will be redirected to a different mirror seamlessly without interruptions. 
File synchronisation is the method of choice for home backups this is preferable over the traditional backup methods where data is simply dumped onto another hard drive. This process prevents copying identical files which leads to faster transfers and a less chance of errors occurring. My personal favourite file synchronisation tool is Unison \cite{unison}. It is also possible to synchronise folders over the network between two home computes, Unison does this through SSH.
Blockchain \cite{blockChain} the method used for secure crypto ledgers which also relies on synchronisation between peers. Synchronisation for the ethereum crypto currency blockchain can be expressed as follows. "Synchronisation proceeds from head to known block by requesting and fetching block hashes iteratively from young to old (head to root). Based on block hashes, blocks can be requested. Based on the parenthash of a block, independent sections can be linked and a chain established. By checking if a block is found in the block chain the root of the blockpool can be established and the chain can be inserted in the blockchain."
%#######################################################
%                                                   
% check if its ok to just dump stuff from the web.
%
%#######################################################

%
%
% End of sync
%
%






%
%
% Stick these somehwere maybe 
%
%
Machine learning has been known to be used for secure communications although it is not clear if it is used in servers with valuable data. Google's AI successfully created secure algorithms \cite{GoogleAi1} that use inhuman cryptographic schemes making them harder to crack. This technique is called GAN Cryptography \cite{GoogleAi2} for which a research paper can be found.

Currently dynamic encryption is exercised in voip phone calls by a company known as Dencrypt \cite{dencrypt} according to the explanation of their proprietary algorithm they use a wrapper on top of AES-256 which is a chosen algorithm that is discarded when the data transfer is finished.


This project will be compatible with NodeJs because it is one of the fastest growing server platform \cite{NodeJs} that can be easily set up and supports a large amount of modules.

%
%
% Stick these somehwere maybe ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
%
%

%
%
% Notes Start
%
%
%Position your topic within Computer Science. This activity will aid you in your literature review also. We zoom out to see three levels:

% notice the enumerate structure to create itemized lists
%\begin{enumerate}
%    \item What is the core topic your project is about? e.g., Mobile app for online voting.
%    \item What core area(s) does the project fall under? e.g., Mobile applications, Social Networking, %Service Providers. 
%    \item What main area(s) of Computer Science does the project fall under? e.g. Software Development, %Cloud Computing.
%\end{enumerate}

%The ACM Computing Classification System (http://www.acm.org/about/class) will aid you in this, use the 2012 categories. Make sure to use figures and illustrations were appropriate. LaTeX will take care of the formatting of these. Do not try to get fancy here, you should concentrate on the content and not the formatting, this is why we are specifying LaTeX.

% Again take note of the structure, simply copy and paste this for future single figures
%\begin{figure}[ht]
%  \centering
%      \includegraphics[width=0.7\textwidth]{successkid.jpg}
%  \caption[A picture of the success kid!]{A picture of the success kid!\cite{Reference1}}
%  \label{fig:successkid}
%\end{figure}

%You can specify the width and label for a figure which allows you to reference the figure and you can attribute a source in the figure caption as is done for figure \ref{fig:successkid}. Make sure you reference all external figures (i.e. figures you did not create yourself). Also use references for all figures e.g. use "... in figure \ref{fig:successkid} ..." NOT "... in the figure above ...".

%\section{Project Scope}
%Project specifics: Background minimum knowledge.

%Imagine you wanted to explain the specifics of your project to a person that knows nothing of Computer Science. You cannot talk about everything (as the idea is not to write a 500+ page report). Remember the reader at this stage can only be assumed to know what you have covered, so identify what are the minimum concepts belonging to the main areas (listed as 3 in the section before) and the core areas (listed as 2 in the section before) that you would need to explain so that the reader is able to understand the specifics of your project and indeed the following section. For example the minimum amount of knowledge about software development, cloud computing, mobile applications, social networking and service providers that are required so as to understand the specifics of a project about a mobile app for an on-line voting system. Here we are making the same trip we did before, but now in the opposite direction. Start zooming in from 3, then to 2 and finally to reach your project 1. Once the reader is finished this section they should be able to understand the proceeding sections (and have context for it within the project).
%
%
% Notes End
%
%

%
%
% Notes 2.2 Start
%
%
%\section{A Review of -INSERT THEMATIC AREA-}
%The focus of this section is at the heart of the project research phase. You must identify the main sources of information you should be aware of within your chosen area and pay regular attention to so as to strengthen your knowledge in the core topic you are working at. So here you should develop an knowledge of not only your core topic but also about the area of computer science the topic falls under. More specifically you should research the following:
%\begin{itemize}
%    \item The top 5 International Conferences and Journals most related to your topic. This is crucial, as it represents the main source for keeping you aware of what the state-of-the-art in your topic is.
%    \begin{itemize}
%        \item In particular it will make you aware of what other projects related to yours have been already done (so that you can compare/position your project w.r.t. these).
%        \item What new techniques are being developed, so that you can apply them in your work. e.g. new frameworks for data visualization
%    \end{itemize}
%    \item The top 3 most recent books/texts related to your topic. There are many free resources from which you may download a relevant text on the topic of your project. Try to either download or borrow 3 recent (no older than 10 years) texts relating to the topic your project is on which you will use throughout the project as reference material and to aid in tackling a number of the technical problems you may encounter. Any PhD/MSc thesis that have published in the last 5 years relating to the topic are also invaluable resources as they will contain a state of the art and references in your project topic. Approach these only after reading/viewing the wikis/Youtube videos you find as a certain level of knowledge will be assumed about the topic.
%    \item The top 5 companies/organizations potentially interested in the product you are developing. Finally, this is also crucial, as it forces you extend to purely programmer view of the project to a wider view considering the market, potential stakeholders and niches where your product can become useful. Moreover, Computer Science is a huge topic with loads of different works and roles. If you pick a project in the area you feel passionate about, and you identify what the market in this area is about, then you can drive your future professional career (from the very beginning) towards the path that makes you happier. I know that this does sound as a very technical reason, but I suppose we all agree is probably the most important of all reasons for choosing a particular project focus. 
%    \item The top 5 wiki/forums/blogs/Youtube channels most related to your topic. This is crucial to you as well, as it represents a more accessible, personal and less informal way of communication with people working/interested on the same topic as you are. This communication is extremely helpful for improving your skills, solving potential doubts and increase the interest/relevance of the topic/area itself.
%\end{itemize}

%You should begin your journey of discovery in reverse order to the listing above (which is given in order of academic importance/significance). So when you are researching your topic first look up some TedX talks or youtube tutorials, then research what companies are doing in the area, then get a handful of very good texts on the core topics of your area (anything older than 5 years usually is not helpful here) and finally start reading conference or journal papers (again newer is better here). In particular during this section you may need to use tables to list resources. These are also automatically formatted in latex thus allowing you to concentrate on content. for example table \ref{tab:Mylar}.

%\begin{table}[ht]
%	\centering
%		\begin{tabular}{ c  c  }
%		\hline
%		\hline
%		Parameter & PET \\
%		\hline
%		Youngs Modulus & 2800-3100MPa \\
%		Tensile Strength & 55-75MPa \\
%		Glass Temperature & 75$^\circ$C \\
%		Density & 1400kg/m$^3$ \\
%		Thermal Conductivity & 0.15-0.24Wm$^{-1}$K$^{-1}$ \\
%		Linear Expansion Coefficient & $7\times10^-5$ \\
%		Relative Dielectric Constant @ 1MHz & 3\\
%		Dielectric Breakdown Strength & 17kVmm$^{-1}$\\
%		\end{tabular}
%	\caption{PET Physical Properties}
%	\label{tab:Mylar}
%\end{table}

%What has been done before in your community w.r.t. your topic? Once you have gotten an understanding of the topic and technologies and have identified the top 5 formal conferences/journals, wiki/forums/blogs/Youtube channels and companies/organizations the next step is to research in depth on them! And here in depth means in depth. Make sure you cite\cite{Reference1} a number of papers \cite{Reference3}, luckily Latex will take care of the ordering of the citations \cite{Reference2} for you.

%The aim here is that you find the trends in your topic (3), and more in general in the area in which your topic resides (2) your project falls under and from these trends you develop your initial project question further and begin to get insights into how others have solved/approached similar problems. Think of this section as colouring in your initial idea. Before you approach this section you should read at least 4/5 good literature reviews (a selection of last years projects will be posted on blackboard to aid you but you should find other sources also).

%In particular in this section, you must find and analyse at least 5 (ideally around 10) works belonging to, or at least related to, your work. You must describe these works and position your project w.r.t. them (i.e., clearly identify the similarities and differences between your project and each of these works). Also remember if you find that you are detailing topics that you have not introduced already here you need to add something to the earlier Scope section.
%
%
% Notes 2.2 End
%
%

%
%
% Research papers
%
%
To achieve my goal of dynamically encrypting messages between both parties, each party must know the encryption key without explicitly sending that key to each other. For this to have a dynamic effect multiple processes will in this case synchronise which each other and a separate process will take care of swapping keys.

The research papers that will be cited synchronise once therefore I will just have to take what I learned and apply it to multiple instances.

Two identical neural networks that originally have a random generated state. This state is different between the two networks and to achieve synchronisation this state must be the same, this is because when the state is the same it will be essentially the key used to encrypt messages. And all this will be achieved without ever sending the key over the network even in an encrypted form as in public private encryption techniques. 

A tree parity machine is a common method used across all papers in order to achieve this goal figure \ref{fig:treeParityMachine} visualises such a machine.

\begin{figure}[ht]
  \centering
      \includegraphics[width=0.7\textwidth]{treeParityMachine.png}
  \caption[Tree parity machine]{Tree parity machine with L=[-4,4], K=3 and N=4\cite{Private_Inputs_to_Tree_Parity_Machine}}
  \label{fig:treeParityMachine}
\end{figure}

According to \cite{Genetic_Key_Guided_Neural_Deep_Learning_based_Encryption} both parties should have the same layout of the tree parity machine where N is the number of neutrons used as input for each hidden neutron. The input neutrons have the X and are at the bottom of the diagram, in this case there are four input neutrons for each hidden neutron. The hidden neutrons are referenced as K, these are the neutrons in the middle with the W in the diagram. These are the weights which are updated if the output of both of the tree parity machines are the same. These weights in the end will be the key used for symmetric encryption between the two parties. T will be the output value which will be compared to the other tree parity machine. 

There is a low number of machine learning algorithms that can be used to synchronise and the paper \cite{Genetic_Key_Guided_Neural_Deep_Learning_based_Encryption} as well as the majority of papers use the hebbian-learning rule figure \ref{fig:hebianFormula}

\begin{figure}[ht]
  \centering
      \includegraphics[width=0.7\textwidth]{hebbian_learning.png}
  \caption[Hebbian learning rule]{Hebbian learning rule\cite{Genetic_Key_Guided_Neural_Deep_Learning_based_Encryption}}
  \label{fig:hebianFormula}
\end{figure}

The other two learning rules that can easily be substituted according to this paper \cite{DESIGN_OF_AN_EFFICIENT_NEURAL_KEY_GENERATION} are as follows anti-hebbian learning rule figure \ref{fig:antihebbianlearning} and random-walk rule figure \ref{fig:randomwalklearning}.

\begin{figure}[ht]
  \centering
      \includegraphics[width=0.7\textwidth]{anti-hebbian.png}
  \caption[Anti-Hebbian learning rule formula]{Anti-Hebbian learning rule formula\cite{DESIGN_OF_AN_EFFICIENT_NEURAL_KEY_GENERATION}}
  \label{fig:antihebbianlearning}
\end{figure}

\begin{figure}[ht]
  \centering
      \includegraphics[width=0.7\textwidth]{random-walk.png}
  \caption[Random-Walk learning rule formula]{Random-Walk learning rule formula\cite{DESIGN_OF_AN_EFFICIENT_NEURAL_KEY_GENERATION}}
  \label{fig:randomwalklearning}
\end{figure}

There are a number of steps involved in synchronising the tree parity machines.
Step 1. The weights at the beginning should be randomly initialised using local randomisation techniques because there is a chance that downloading data from random APIs can be spoofed which will result in the attacker easily synchronising with your tree parity machine. 

Step 2. Generate random input which will be used by both of the tree parity machines. This input can be generated by a third party server or one of the two parties.

Step 3. Calculate the value of the weights based on the random input using the formula in figure \ref{fig:hiddenNeutronFormula}

\begin{figure}[ht]
  \centering
      \includegraphics[width=0.7\textwidth]{hidden_neutron_folmula.png}
  \caption[Hidden neutron formula]{Hidden neutron formula\cite{Genetic_Key_Guided_Neural_Deep_Learning_based_Encryption}}
  \label{fig:hiddenNeutronFormula}
\end{figure}

Step 4. Calculate the output neutron based on the weights using the formula in figure \ref{fig:outputNeutronFormula}

\begin{figure}[ht]
  \centering
      \includegraphics[width=0.7\textwidth]{outputNeutronFormula.png}
  \caption[Output Neutron Formula]{Output Neutron Formula\cite{Genetic_Key_Guided_Neural_Deep_Learning_based_Encryption}}
  \label{fig:outputNeutronFormula}
\end{figure}

Step 5. exchange the output between both of the tree parity machines through a network and if the outputs are not the same repeat again from step 2. And if the outputs are the same apply the hebbian learning rule to the weights and update them accordingly. Repeat step 3 to step 5 until both of the tree parity machines have the same weights.

This technique can be considered too simplistic and the attacker has a greater probability in synchronising with the two parties. I will cover attacks related to tree parity machines further in the section.

To increase the speed of synchronisation and lower the chance of the attacker being able to synchronise with the parties. A technique called queries is used \cite{Private_Inputs_to_Tree_Parity_Machine}.
This technique replaces step 2 from before where the input would be randomly generated by a third party or one of the party. A query consists of a generated vector based on a field of the weights. These queries are then sent from each party to the other interchangeably. 
To calculate the new local field value the following formula can be used figure \ref{fig:LocalFieldFormula}
\begin{figure}[ht]
  \centering
      \includegraphics[width=0.7\textwidth]{syncQueriesLocalField.png}
  \caption[Local Field Formula]{Local Field Formula\cite{Private_Inputs_to_Tree_Parity_Machine}}
  \label{fig:LocalFieldFormula}
\end{figure}

It is possible to use a different algorithm where the output \[\sigma_k \] is chosen random for the hidden unit.
It is possible to use the following formula to calculate the local field value. \[ h_k = \sigma_k H \]

To calculate the list of c values that will be used to affect the generation of input values it is possible to use the following two formulas. 
\begin{figure}[ht]
  \centering
      \includegraphics[width=0.7\textwidth]{cformulaa.png}
  \caption[C Formula one]{C Formula one\cite{Private_Inputs_to_Tree_Parity_Machine}}
  \label{fig:LocalFieldFormula}
\end{figure}
\begin{figure}[ht]
  \centering
      \includegraphics[width=0.7\textwidth]{cformulab.png}
  \caption[C Formula two]{C Formula two\cite{Private_Inputs_to_Tree_Parity_Machine}}
  \label{fig:LocalFieldFormula}
\end{figure}

Where one formula is chosen randomly in each calculation. The probability of one formula being chosen is 50 percent.
Th inputs are then generated and if the inputs are associated with zero weights then the inputs are randomly generated since they do not influence the local field value. 
The input is then divided into L groups and are selected randomly and assigned to \[x_k,_j = sgn(W_k_j) \]
% this double subscript error is fine displays as it should
Finally the remaining input values are set to  \[x_k,_j = -sgn(W_k,_j) \]
To achieve secure key exchange with queries the parties should choose the parameter H so that they can synchronise quickly while the attacker would not be able to do so in time.
Despite the queries being based on the weights an attacker cannot predict the query generated by either party since the weights are never shared. Therefore an attacker can collect the queries but won't be able to establish a mathematical connection between them.
After the synchronisation is complete the weights can be used as a seed for a random generator. As the attacker doesn't know this seed the output of this generator won't be predicted.

There are a number of different attacks that can be carried out on the tree parity machines. These attacks are  more successful if using the basic synchronisation method without the use of queries. This is because up to this date there is no documented or rumoured methods of being able to synchronise with parties who are using queries.

The most basic and useless attack would be a brute force.
This involves trying every possible values for the weights. If using a tree parity machine consisting of 3 hidden neurons, 300 input neurons and 3 weights will result \cite{Private_Inputs_to_Tree_Parity_Machine} in \[3*10^2^5^3\] possible values for the weights making this attack quite impossible using modern computing power. 

The attacker can attempt to learn the weights by using their own tree parity machine with the same number of hidden neutrons and inputs \cite{Private_Inputs_to_Tree_Parity_Machine}. This is essentially identical to the parties tree parity machines but with different initial weights and the attacker synchronises indirectly.
There are three possible situations that can occur with this attack. In these situations A and B will be the two parties trying to synchronise and E is the attacker.
situation 1. Output of A doesn't match output of B and therefore none of the parties including the attacker update their weights.
situation 2. Output of A matches output of B and output of E is also the same. This time A, B and E update their weights.
situation 3. Output of A matches output of B but output of E doesn't match. parties A and B update their weights but E cannot do that and therefore it will take E more time to synchronise to A than it would for B to synchronise with A. Because the learning is ceased after A and B are synchronised E will be left in the dark and not synchronised state. You can further decrease the success rate of this attack by increasing the synaptic depth of the neural network \cite{Private_Inputs_to_Tree_Parity_Machine}. Increasing this will have a performance impact on the parties polynomially however the chance of successful attack decreases exponentially.

More sophisticated attacks can be found in this research paper \cite{BIG_ResearchPaper} such as the genetic attack.
This uses a form of genetic algorithm the key to a successful attack using this method revolves around E being able to determine the fitness of her neural networks.
The attacker spawns a number of tree parity machines which attempt to synchronise with A. After a set time period a selection is made where the most successful synchronised tree parity machines are used to generate the next population. This selection works best if there are certain observable differences between attractive and repulsive effects. 
This attack can be quite successful if the synaptic depth of the neural network is not too large. This can be expressed with the following formula in figure \ref{fig:geneticESuccess} where the probability of E being successful decreases exponentially with the synaptic depth L.
\begin{figure}[ht]
  \centering
      \includegraphics[width=0.3\textwidth]{geneticFormula0.png}
  \caption[probability of E being successful]{probability of E being successful\cite{BIG_ResearchPaper}}
  \label{fig:geneticESuccess}
\end{figure}

In order for this attack to be likely successful the number of tree parity machines the attacker will use will need to be exponentially increased with the increase of the synaptic depth see figure \ref{fig:geneticEMachine}.
\begin{figure}[ht]
  \centering
      \includegraphics[width=0.3\textwidth]{geneticFormula1.png}
  \caption[Tree Parity Machines needed for synaptic depth]{Tree Parity Machines needed for synaptic depth\cite{BIG_ResearchPaper}}
  \label{fig:geneticEMachine}
\end{figure}

The most successful attack for the simple tree parity machine is the majority attack \cite{BIG_ResearchPaper}. The genetic attack works better than the majority attack if the synaptic depth is not large. The majority attack uses multiple tree parity machines just like the genetic attack however there is no selection of the fittest tree parity machines. This way the number of tree parity machines is constant. This attack uses the Bayes learning rule \cite{Bayes_rule} to find the overlap between the hidden units of one of the parties network and the attackers network.
It is possible to use the following formula in figure \ref{fig:majority0} to calculate the probability of synchronisation. 
\begin{figure}[ht]
  \centering
      \includegraphics[width=0.5\textwidth]{majorityAttack0.png}
  \caption[probability of E being successful]{probability of E being successful\cite{BIG_ResearchPaper}}
  \label{fig:majority0}
\end{figure}
At the beginning of the attack the probability of the attacker being successful is 0 and when the attack is successful the probability is 1.
The average overlap between two of the attackers networks can be calculated using the following formula figure \ref{fig:majority1}.
\begin{figure}[ht]
  \centering
      \includegraphics[width=0.7\textwidth]{majorityAttack1.png}
  \caption[probability of overlap]{probability of overlap\cite{BIG_ResearchPaper}}
  \label{fig:majority1}
\end{figure}
If the result of this formula is 1 then the two of the attackers networks tested are identical which can reduce the majority attack to the geometric method.
If trying to attack a tree parity machine with 3 hidden neutrons, 1000 input neutrons and a synaptic depth of 5 it is possible to successfully synchronise at around 1000 steps. this is done with only 100 attacking networks. Figure \ref{fig:majority1} demonstrates the this attack over 1000 steps.
\begin{figure}[ht]
  \centering
      \includegraphics[width=0.8\textwidth]{majorityAttack2.png}
  \caption[Majority attack results]{Majority attack results\cite{BIG_ResearchPaper}}
  \label{fig:majority2}
\end{figure}
%
%
% End of research papers
%
%

%
%
% Start of companies
%
%
My project falls mostly under cryptography and secure transfer of information therefore the companies that could be potentially interested would need to be involved in that market. 
Google is known for the many cloud services it provides like gmail, docs and drive. The data needs to be encrypted before sending it to the user to display. When using the products Google claims to use several layers of encryption to protect customer data \cite{googleSecurity}. Google has its own cryptographic library called Tink \cite{googleTink}, ensuring that they have an interest for cryptography development. Google also takes part in less conventional methods of cryptography like using their own neural network to come up with obscure encryption methods \cite{GoogleAi1}. The encryption methods my project uses integrate a machine learning algorithm which could peak the interest of Google.

Dencrypt \cite{dencrypt} is a company that uses dynamic encryption for voice over IP communication. They use a wrapper around AES-256 to achieve the dynamic nature. They claim dynamic encryption is the state of the art in cryptology and I believe they would be interested in different methods of achieving dynamic encryption. Dencrypt explains that even if someone manages to decrypt a single data transaction the next transaction would not reveal information as it is encrypted in a different way, This is exactly what I'm aiming to achieve with my project just using different methods. 

Baffle \cite{Company_baffle} offers advanced data protection solutions for financial services, healthcare, secure cloud migrations and saas providers. Their product also protects data while it is in memory and while it is processing this means they are very serious about data protection and try to close gaps in the data threat model. In order to achieve this they use a number of different methods to secure data as seen in figure \ref{fig:baffle}.
\begin{figure}[ht]
  \centering
      \includegraphics[width=0.8\textwidth]{babbleHowItWorks.png}
  \caption[How does Baffle work]{How does Baffle work\cite{Company_baffle}}
  \label{fig:baffle}
\end{figure}
The project on which I'm working on could be useful to Baffle when transferring data between servers. Baffle attempts to avoid legacy encryption solutions and favours more advanced versions which my project may peak their interest.

IBM offers many cloud products that require encryption for security purposes. IBM Guardium \cite{IBM_Guardium} is a product that is purposely built for encrypting files and databases. This features centralised key and policy management as well as granular encryption of files and folders each protected with its own encryption key. The same is true for volumes of data. There is a clear indication that good security results in the use of multiple keys. I believe IBM could be interested in some sort of dynamic encryption perhaps not for IBM Guardium as that is mainly for safe storage and my project revolves around secure transfer of data between hosts, but in one or more of their many other products.

Dell has a product called Encryption Enterprise \cite{Dell_Encryption_Enterprise} which handles everything like file encryption, malware prevention and protecting data in motion. Dell also takes pride in the fact that this product integrates well with other solutions like Bitlocker. The Encryption Enterprise software is also used by the government, healthcare and other critical infrastructure ensuring its high reputation status. The data in motion would relate to my project which could be of interest to Dell as data sent between hosts needs to be encrypted for maximum security. 
%
%
% End of companies
%
%